{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fc100-507b-4f4b-984a-42252b1497b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import xarray as xr\n",
    "\n",
    "import io\n",
    "import cmdstanpy\n",
    "import arviz as az\n",
    "from IPython.display import Image\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.layouts import gridplot, row, column\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "from bokeh import palettes\n",
    "from bokeh.models import Legend\n",
    "from scipy import odr\n",
    "\n",
    "\n",
    "from bokeh.palettes import Spectral6\n",
    "from bokeh.models import ColorBar, ColumnDataSource\n",
    "from bokeh.transform import linear_cmap\n",
    "\n",
    "from IPython.display import Image\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "bokeh.io.output_notebook()\n",
    "import holoviews as hv\n",
    "import bebi103\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d02014-f1f4-486f-80ff-dd2bea68d82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Determine GEOS-Chem HEMCO Scaling factor and incorporating model intelligence for ethane and propane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbe561-85dc-4717-b3dd-d089d111363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ATOM = pd.read_csv('../../../data/processing/geoschem_plane_sample_ATOM.csv')\n",
    "plane_HIPPO = pd.read_csv('../../../data/processing/geoschem_plane_sample_HIPPO.csv')\n",
    "\n",
    "plane_ATOM_n2ofilt_100tropres = pd.read_csv('../../../data/geos_chem_output/plane_ATOM_n2ofilt_100tropres.csv')\n",
    "\n",
    "gc_atom_synop_n2ofilt_theta_itp_100tropres = pd.read_csv('../../../data/geos_chem_output/gc_atom_synop_n2ofilt_theta_itp_100tropres.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087afe-2aa2-41d5-bd51-4575b3fc0100",
   "metadata": {},
   "source": [
    "What is the max and min theta for HIPPO during the winter compared to the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e38cd-fc5a-4ed6-a417-bc377200999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplott = plane_HIPPO.loc[plane_HIPPO['campaign_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fa417-32ef-47b5-a471-73bee02651b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 350\n",
    "fw = 350\n",
    "p = bokeh.plotting.figure(frame_height=fh, frame_width=fw, title='')\n",
    "\n",
    "p.circle(dfplott.lat,dfplott.theta)\n",
    "p.xaxis.axis_label='lat'\n",
    "p.yaxis.axis_label='theta'\n",
    "p.title = 'filtered HIPPO 1'\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bf7b6-deb5-44da-8e41-cabbb2a5cc11",
   "metadata": {},
   "source": [
    "## HCN filter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c75794-d4f9-473e-8ad3-3eab73a75257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pertcile_indx(array,percentile):\n",
    "    \"\"\" Array is a list of gas measurements. \n",
    "    Percentile is the maximum percentile the value should \n",
    "    be equal to or greater than to return the indices for.\n",
    "    \"\"\"\n",
    "    percval = [st.stats.percentileofscore(array, x, 'weak') for x in array]\n",
    "    return [i for i, j in enumerate(percval) if j >= percentile]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e48deb-16ed-41a9-863d-4946e580c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pctile_ind = pertcile_indx(plane_ATOM_n2ofilt_100tropres.copy().hcn.to_list(),87)\n",
    "dfp = plane_ATOM_n2ofilt_100tropres.copy().iloc[pctile_ind]\n",
    "\n",
    "fh = 400\n",
    "fw = 400\n",
    "p = bokeh.plotting.figure(frame_height=fh, frame_width=fw, title='')\n",
    "p.circle(plane_ATOM_n2ofilt_100tropres.c2h6.values*1e3,\n",
    "     plane_ATOM_n2ofilt_100tropres.hcn.values*1e3, size=5, color='red')\n",
    "p.circle(dfp.c2h6.values*1e3,dfp.hcn.values*1e3, size=5, color='black')\n",
    "p.xaxis.axis_label = \"C‚ÇÇH‚ÇÜ (ppb)\"\n",
    "p.yaxis.axis_label = \"HCN (ppb)\"\n",
    "p.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.xaxis.major_label_text_font_size = \"15pt\"\n",
    "p.yaxis.major_label_text_font_size = \"15pt\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.yaxis.major_tick_line_width = 3\n",
    "p.axis.axis_label_text_font_style = 'bold'\n",
    "# p.legend.label_text_font_size = '14pt'\n",
    "\n",
    "\n",
    "# p.legend.location = \"bottom_right\"\n",
    "bokeh.io.show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b13ec1-d843-49e0-a5a3-63211b6f4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pctile_ind = pertcile_indx(plane_ATOM_n2ofilt_100tropres.copy().hcn.to_list(),87)\n",
    "dfp = plane_ATOM_n2ofilt_100tropres.copy().iloc[pctile_ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac770f7e-6179-46c1-acc6-96c7eed19abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ATOM_filt_hcnfilt = plane_ATOM_n2ofilt_100tropres.copy()\n",
    "plane_ATOM_filt_hcnfilt.loc[pctile_ind,['c2h6','hcn','c3h8']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcc1a7-6259-4890-bc70-5623adc52e2c",
   "metadata": {},
   "source": [
    "Check that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a536ee-0851-4684-abd4-076441292599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ATOM_filt_hcnfilt['c2h6'][pctile_ind].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc2c6d-590b-48b0-a0c0-0faa3c7ff45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plane_ATOM_filt_hcnfilt[['c2h6','hcn','c3h8']].apply(lambda x: x.fillna(method='backfill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fed7cf-67a7-4727-a350-f1d7bdf50959",
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ATOM_filt_hcnfilt['c2h6'] = res['c2h6']\n",
    "plane_ATOM_filt_hcnfilt['hcn'] = res['hcn']\n",
    "plane_ATOM_filt_hcnfilt['c3h8'] = res['c3h8']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909f111-700a-4cfe-b6df-ce17a290757a",
   "metadata": {},
   "source": [
    "Get the slope of the wintertime for c3h8 vs c2h6 and c3h8 vs ch4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13a76d-65dc-463d-aa2c-7d7f1cbf1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_winter = plane_ATOM_filt_hcnfilt.loc[plane_ATOM_filt_hcnfilt['campaign_id'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a931c-0b87-4750-ab8c-aa93b96c3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = odr.Data(atom_winter.c2h6, atom_winter.c3h8)\n",
    "odr_obj = odr.ODR(data, odr.unilinear)\n",
    "output = odr_obj.run()\n",
    "x_fit = np.linspace(min(atom_winter.c2h6), max(atom_winter.c2h6), 1000)\n",
    "y_fit = output.beta[0]*x_fit + output.beta[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92a14a-d891-4dc0-830e-db3043100422",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title='winter atom all curtains', width=400, height=400)\n",
    "p.xaxis.axis_label = 'c2h6'\n",
    "p.yaxis.axis_label = 'c3h8'\n",
    "p.circle(atom_winter.c2h6, atom_winter.c3h8, color='red', size=5, line_alpha=0)\n",
    "p.line(x_fit, y_fit, line_width=2)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ad8db-d3e5-450c-9f80-16d9ae3140ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the slope \n",
    "output.beta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540cd91-ebf4-494c-bb91-506e3d4422cc",
   "metadata": {},
   "source": [
    "#### Hierchical Bayesian Model\n",
    "\n",
    "We can reasonably approximate C2H6 and C3H8 measured by the aircraft/GEOS-Chem to be Lognormally distributed with an approximate error. Lognormal distributions have longer tails, which is appropriate given the outliers we see in the measurements. We can \"model\" the GEOS-Chem simulated results as follows: \n",
    "\n",
    "\\begin{align}\n",
    "&&\\mathrm{gcs}_i &\\sim \\text{LogNorm}(\\beta \\cdot \\mathrm{a}_i, \\sigma)\\\\\n",
    "&&\\sigma &\\sim \\text{Prior}\\\\\n",
    "&&\\beta &\\sim \\text{Prior}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Where \"gc_i\" represents GEOS-Chem simulated C3H8 and C2H6, \"aircraft_i\" represents measured C3H8 or C2H6 measured by the aircraft. Here, $\\beta$ is the scalar we are interested in, that estimates missing HEMCO emissions, and $\\sigma$ is the approximate error in GEOS-Chem simulations. \n",
    "\n",
    "We \"repeated\" this experiment with several GEOS-Chem replicates over several days before and after the aircraft path, as explained above. If we were to pool all the data together, each experiment would be governed by identical parameters. However, we know that each replicate is subject to differences mainly due to meterology, and we conclude that the parameters in each replicate experiment should be independent from one another, such that we have k separate models to fit, each looking like the one above.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8868a9-b158-4d99-9edd-2114c6e2a694",
   "metadata": {},
   "source": [
    "We can consider a model in which there is a \"master\" scaling parameter, which we now call $\\beta$, and the values of the scaling parameters of the replicates, which we now call $\\beta_1$, may vary from this $\\beta$ according to some probability distribution, $g(\\beta_1{_i} | \\beta)$. So now, we have parameters $\\beta_1{_{,1}}$, $\\beta_1{_{,2}}$, ... $\\beta_1{_{,i}}$ and $\\beta$.  The posterior can be written using Bayes's theorem, defining $\\beta_1 = (\\beta_1{_{,1}}, \\beta_1{_{,2}}, ...)$,\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\beta, \\beta_1 | \\mathrm{a}, \\mathrm{gcs}) = \\frac{f(\\mathrm{a}, \\mathrm{gcs} | \\beta, \\beta_1)g(\\beta, \\beta_1)}{f(\\mathrm{a}, \\mathrm{gcs})}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note though, that the observed values of gc do not directly depend on $\\beta$, only on $\\beta_1$ and as such, the observations are only indirectly dependent on $\\beta$. So, we can write: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51045768-7a11-4438-ad9f-d5a8fed45498",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "g(\\beta, \\beta_1 | \\mathrm{a}, \\mathrm{gcs}) = \\frac{f(\\mathrm{a}, \\mathrm{gcs} | \\beta_1)g(\\beta, \\beta_1)}{f(\\mathrm{a}, \\mathrm{gcs})}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Next, we can rewrite the prior using the definition of conditional probability:\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\beta, \\beta_1) = g(\\beta_1 | \\beta)g(\\beta)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Substituting this back into the previous expression for the posterior, we have: \n",
    "\n",
    "\\begin{align*}\n",
    "g(\\beta, \\beta_1 | \\mathrm{a}, \\mathrm{gcs}) = \\frac{f(\\mathrm{a}, \\mathrm{gcs} | \\beta_1)g(\\beta_1 | \\beta)g(\\beta)}{f(\\mathrm{a}, \\mathrm{gcs})}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad28f4f-082c-4270-a940-1ec03028b077",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the numerator, we see a chain of dependencies. The gc simulations depend on $\\beta_1$. Parameters $\\beta_1$ depend on hyperparameter $\\beta$. Hyperparameter $\\beta$ then has some hyperprior distribution. So, the hierarchical model captures both the experiment-to-experiment variability, as well as the master parameter. \n",
    "\n",
    "We have to specify a hyperprior, and a conditional prior, $g(\\beta_1 | \\beta)$. Here, we have no reason to believe that we can distinguish any one $\\beta_1{_i}$ from another prior to the experiment. As such, we can assume the conditional prior to behave in an exchangeable manner, where the label \"i\" is not dependent on the permutations of the indices. So, our expression for the posterior is: \n",
    "\n",
    "\\begin{align}\n",
    "g(\\beta_1, \\beta \\mid \\mathrm{a}, \\mathrm{gcs}) = \\frac{f(\\mathrm{a}, \\mathrm{gcs}\\mid \\beta_1)\\,\\left(\n",
    "\\prod_{i=1}^kg(\\beta_{1,i}\\mid \\beta)\\right)\\,g(\\beta)}{f(\\mathrm{a}, \\mathrm{gcs})}\n",
    "\\end{align}\n",
    "\n",
    "We assume that all replicates have the same scale parameter, $\\tau$, but differing location parameters. Our statistical model is then defined as follows, with a weakly informative prior on $\\tau$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6078d4-80d8-41d0-8fe2-a3ac3d8a0005",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&\\tau_{i} = 0.05{\\left|\\Delta t\\right|}_i + 0.01 \\\\\n",
    "&\\beta \\sim \\text{Norm}(0.7, 0.2)\\\\\n",
    "&\\beta_{1,i} \\sim \\text{Norm}(\\beta, \\tau_{i})\\\\\n",
    "&\\alpha = 1/\\beta\\\\\n",
    "&\\sigma_{i} = 0.14\\cdot \\mathrm{tropht}_i + 0.8\\\\\n",
    "&\\mathrm{gcs}_i \\sim \\text{LogNorm}(\\beta_{1,i} \\cdot \\mathrm{a}_i, \\sigma_i)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55c54a-74e9-4b14-9144-7033dc98998d",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&&\\mathrm{a} & = \\mathrm{gcs}\\cdot \\alpha\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9a554-8d7e-4134-8c90-4321619fa001",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "g(\\alpha) = \\prod_{s=1}^k g(\\alpha{_s},\\alpha{_{1,s}} \\mid a_s,gc_s)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815280c9-f29e-460d-9622-9aee053bf0f8",
   "metadata": {},
   "source": [
    "For campaign number i, and measurement j:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e64a5f-b397-44a9-8736-de9818f03bbd",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&\\tau_{ij} = 0.05{\\left|\\Delta t\\right|}_{ij} + 0.01 \\\\\n",
    "&\\beta_{i} \\sim \\text{Norm}(0.7, 0.2)\\\\\n",
    "&\\beta_{1,ij} \\sim \\text{Norm}(\\beta_{i}, \\tau_{ij})\\\\\n",
    "&\\alpha_{i} = 1/\\beta_{i}\\\\\n",
    "&\\sigma_{ij} = 0.14\\cdot \\mathrm{tropht}_{ij} + 0.8\\\\\n",
    "&\\mathrm{gcs}_{ij} \\sim \\text{LogNorm}(\\beta_{1,ij} \\cdot \\mathrm{a}_{ij}, \\sigma_{ij})\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015dd59-ed29-4df6-ac86-3203622b4cf2",
   "metadata": {},
   "source": [
    "These are functions for tau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73523b40-c882-4d12-bbdf-2a83dbc965f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_t(synlabel, plane_path_day_num):\n",
    "    \"\"\" Takes in the synoptic_day_number variable from your gc output.\n",
    "    This datatype should be an integer. Doesn't take in an array - does it one by one.\n",
    "    The plane path day number is the number of the synoptic replicates that the plane lands on.\n",
    "    Ie 2.\n",
    "    \"\"\"\n",
    "    return synlabel - plane_path_day_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f04b1-f136-46bc-9812-d025c31ecc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [0,1,2,3,4]\n",
    "[get_delta_t(r,2) for r in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a552ef-571e-4a98-a868-f4b3f29711bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tao_mean(a,k,delta_t):\n",
    "    \"\"\" function that returns tau, which is a function of time and \n",
    "    the prior for your bayes. This won't go into stan, \n",
    "    I just want to get an idea of how it works so I can refine what the slope should be.\n",
    "    a stands for the slope, and k stands for the vertical shift (tau_0)\n",
    "    \n",
    "    delta_t should be a numpy array and a and k should be scalars\n",
    "    \n",
    "    \n",
    "    **** This function was written so that it could either get tau final, or the new mean for \n",
    "    a distribution of tau in stan. If you input k = 0, then you are doing the latter.\n",
    "    \"\"\"\n",
    "\n",
    "#     return [(a*abs(dt) + k) for dt in delta_t]\n",
    "    return a*abs(delta_t) + k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6468f5c9-9337-4dd3-8bad-1c83552fcf56",
   "metadata": {},
   "source": [
    "Remember, the default is +/-5 days. When you actually do the fits, you limit to +/-2 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e4d34-2e4d-49c9-9280-435c1c151f4a",
   "metadata": {},
   "source": [
    "If you are confused you can plot this to see tao for all datapoints.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd5aee-088d-4c78-90aa-2bd5fce40ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_syn_day_label(synday):\n",
    "    \n",
    "    if synday == 0:\n",
    "        newday = -5\n",
    "    elif synday == 1:\n",
    "        newday = -4\n",
    "    elif synday == 2:\n",
    "        newday = -3\n",
    "    elif synday == 3:\n",
    "        newday = -2\n",
    "    elif synday == 4:\n",
    "        newday = -1\n",
    "    elif synday == 5:\n",
    "        newday = 0\n",
    "    elif synday == 6:\n",
    "        newday = 1\n",
    "    elif synday == 7:\n",
    "        newday = 2\n",
    "    elif synday == 8:\n",
    "        newday = 3\n",
    "    elif synday == 9:\n",
    "        newday = 4\n",
    "    elif synday == 10:\n",
    "        newday = 5\n",
    "        \n",
    "    return newday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ae11a-8b4b-456c-8a2f-455551f25c6d",
   "metadata": {},
   "source": [
    "This looks exactly as you would expected to given the number of default synoptic days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e045ad7-733f-4796-9ac9-9ce15a1ad480",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_atom_synop_n2ofilt_theta_itp_100tropres['tau_dist_means'] = tao_mean(.05,.05,get_delta_t(gc_atom_synop_n2ofilt_theta_itp_100tropres['synoptic_day_number'].values, 5))\n",
    "gc_atom_synop_n2ofilt_theta_itp_100tropres.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeeba5d-f4a7-4e48-a26e-75e086b712fe",
   "metadata": {},
   "source": [
    "Great! Before move on, here are the other priors just as a reminder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983edbc8-d250-431f-9ead-78ab479b27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 300\n",
    "fw = 300\n",
    "p = bokeh.plotting.figure(frame_height=fh, frame_width=fw, title='')\n",
    "p.line(np.linspace(0, 2), st.norm.pdf(np.linspace(0,2), .7, .2), line_width=2, color='black')\n",
    "p.xaxis.axis_label = \"Œ≤\"\n",
    "p.yaxis.axis_label = \"PDF\"\n",
    "p.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.xaxis.major_label_text_font_size = \"15pt\"\n",
    "p.yaxis.major_label_text_font_size = \"15pt\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.yaxis.major_tick_line_width = 3\n",
    "p.axis.axis_label_text_font_style = 'bold'\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2c76a-543b-4777-9e55-0b5c9c589f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(st.norm.ppf(0.01, .7, .2))\n",
    "print(st.norm.ppf(0.99, .7, .2))\n",
    "print(st.norm.ppf(0.5, .7, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91a47a-7c11-4579-8637-f3081c860a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = hv.Points(\n",
    "    data = gc_atom_synop_n2ofilt_theta_itp_100tropres,\n",
    "    kdims=['theta','c3h8']\n",
    ").opts(padding=.1, title='geos chem')\n",
    "\n",
    "p2 = hv.Points(\n",
    "    data = gc_atom_synop_n2ofilt_theta_itp_100tropres,\n",
    "    kdims=['theta','c2h6']\n",
    ").opts(padding=.1)\n",
    "\n",
    "p1+p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9df797-9826-4a2f-b317-a535715aa939",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = hv.Points(\n",
    "    data = plane_ATOM_n2ofilt_100tropres,\n",
    "    kdims=['theta','c3h8']\n",
    ").opts(padding=.1, title='plane')\n",
    "\n",
    "p2 = hv.Points(\n",
    "    data = plane_ATOM_n2ofilt_100tropres,\n",
    "    kdims=['theta','c2h6']\n",
    ").opts(padding=.1)\n",
    "\n",
    "p1+p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665371bd-8da2-4ff3-8452-795db6ee9853",
   "metadata": {},
   "source": [
    "So geos chem propane ranges from 5e-6 to 1.4e-3. (ethane ranged higher from 2e-4 to 2.5e-3). This will help inform your sigma for your lognormal. The mu should be around 2e-4 (ethane was 7e-4 : 1e-3), but this is going to be determined by beta*plane in the actual model, but just for trying things, use a median between that range for mu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ab37f-ade0-4721-875e-05647ed00290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(st.lognorm.ppf(0.01, 3.5, scale=np.exp(.0002)), \"e-4\")\n",
    "print(st.lognorm.ppf(0.99, 3.5, scale=np.exp(.0002)), \"e-4\")\n",
    "print(st.lognorm.ppf(0.01, 3.5, scale=np.exp(.001)), \"e-3\")\n",
    "print(st.lognorm.ppf(0.99, 3.5, scale=np.exp(.001)), \"e-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cbdea-7833-4817-9683-c60ad3428466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_generator_function2(val, tropht_max, tropht_min, base_sigma, sigma_interval_increase): \n",
    "    \"\"\"    \n",
    "    By plotting the tropht of theta bins, I saw that the largest tropht \n",
    "    was about 17km, and the smallest is about 6km. I created 5 intervals to assign a sigma to, which \n",
    "    is about every 2km.\n",
    "    \n",
    "    6km\n",
    "    8km\n",
    "    10km\n",
    "    12km\n",
    "    14km\n",
    "    17km\n",
    "    \n",
    "    Each interval is between those vals (like between 6 and 8) etc.\n",
    "    \"\"\"\n",
    "    interval_increase_kilometers = (tropht_max - tropht_min)/5\n",
    "    \n",
    "    if (tropht_min + interval_increase_kilometers) > val >= tropht_min:\n",
    "        return base_sigma + sigma_interval_increase\n",
    "    \n",
    "    elif (tropht_min + interval_increase_kilometers*2) > val > (tropht_min + interval_increase_kilometers):\n",
    "        return base_sigma + sigma_interval_increase*2\n",
    "    \n",
    "    elif (tropht_min + interval_increase_kilometers*3) > val > (tropht_min + interval_increase_kilometers*2):\n",
    "        return base_sigma + sigma_interval_increase*3\n",
    "    \n",
    "    elif (tropht_min + interval_increase_kilometers*4) > val > (tropht_min + interval_increase_kilometers*3):\n",
    "        return base_sigma + sigma_interval_increase*4\n",
    "    \n",
    "    elif val > (tropht_min + interval_increase_kilometers*4):\n",
    "        return base_sigma + sigma_interval_increase*5  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1b020-885c-4a77-bdfa-66a7c2eb55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "j = -1\n",
    "\n",
    "gc_atom_synop_itp_df = gc_atom_synop_n2ofilt_theta_itp_100tropres.loc[(gc_atom_synop_n2ofilt_theta_itp_100tropres['campaign_id'] == i) & \n",
    "                      (gc_atom_synop_n2ofilt_theta_itp_100tropres['flight_lat_gradient'] == j) & \n",
    "                      (gc_atom_synop_n2ofilt_theta_itp_100tropres['synoptic_day_number'] <= 7) & \n",
    "                        (gc_atom_synop_n2ofilt_theta_itp_100tropres['synoptic_day_number'] >= 3)].copy()\n",
    "\n",
    "plane_df = plane_ATOM_filt_hcnfilt.loc[(plane_ATOM_filt_hcnfilt['campaign_id'] == i) & \n",
    "                   (plane_ATOM_filt_hcnfilt['flight_lat_gradient'] == j)].copy()\n",
    "\n",
    "\n",
    "# get sigma for likelihood using tropht\n",
    "max_tropht = np.max(plane_df.tropht.values)\n",
    "min_tropht = np.min(plane_df.tropht.values)\n",
    "plane_df['sigma_likelihood'] = np.array([sigma_generator_function2(x,max_tropht,min_tropht,.8,.14) for x in plane_df.tropht.values])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f5c8f-a2e8-4385-8e8f-a488bf6516f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 300\n",
    "fw = 300\n",
    "p = bokeh.plotting.figure(frame_height=fh, frame_width=fw, title='')\n",
    "p.circle(plane_df.tropht.values, plane_df['sigma_likelihood'].values, size=4, color='black')\n",
    "p.xaxis.axis_label = \"Tropopause height (km)\"\n",
    "p.yaxis.axis_label = \"œÉ\"\n",
    "p.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.xaxis.major_label_text_font_size = \"15pt\"\n",
    "p.yaxis.major_label_text_font_size = \"15pt\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.yaxis.major_tick_line_width = 3\n",
    "p.axis.axis_label_text_font_style = 'bold'\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd1b186-328a-4537-bdab-a6b242af405c",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c0ffc-4a89-40e1-b31d-9dacd8569de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bebi103.stan.clean_cmdstan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11247d74-3f24-4c6b-9c40-e61c230f2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = cmdstanpy.CmdStanModel(stan_file=\"stan_model.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed656fe-79de-4d00-8282-20986896d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.code())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c8e49-34e5-4701-92ce-3466879daeec",
   "metadata": {},
   "source": [
    "Remember that if you get nans here, it is because you are taking the log of a negative number. Here, it looks good. If you end up getting negatives, you can adjust beta prior and shrink the variance so you don't get a negative tail (move mean over to the right and also decrease the variance). \n",
    "\n",
    "Here are the references you used to get the epsilon \n",
    " https://mc-stan.org/docs/2_26/stan-users-guide/truncated-data-section.html and https://mc-stan.org/docs/2_26/reference-manual/program-block-transformed-parameters.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cd1a2-2804-470a-9b09-b38d427a8ff2",
   "metadata": {},
   "source": [
    "### Posterior Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dde9f-a0ff-46ec-9de3-a15f3ac9f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_error_bars(means, confs, names, **kwargs):\n",
    "    \"\"\"Make a horizontal plot of means/conf ints with error bars.\"\"\"\n",
    "    frame_height = kwargs.pop(\"frame_height\", 150)\n",
    "    frame_width = kwargs.pop(\"frame_width\", 450)\n",
    "\n",
    "    p = bokeh.plotting.figure(\n",
    "        y_range=names, frame_height=frame_height, frame_width=frame_width, **kwargs\n",
    "    )\n",
    "\n",
    "    p.circle(x=means, y=names)\n",
    "    for conf, name in zip(confs, names):\n",
    "        p.line(x=conf, y=[name, name], line_width=2)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec167f2e-61b7-4710-b727-014bee001fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_atom_synop_itp = gc_atom_synop_n2ofilt_theta_itp_100tropres.copy()\n",
    "plane_ATOM_n2ofilt = plane_ATOM_filt_hcnfilt.copy()\n",
    "\n",
    "\n",
    "c_name = []\n",
    "c_id = []\n",
    "c_flg = []\n",
    "species = []\n",
    "alpha_lower = []\n",
    "alpha_median = []\n",
    "alpha_upper = []\n",
    "alpha_path = []\n",
    "ppc_l = []\n",
    "alpha_samples = []\n",
    "\n",
    "\n",
    "for i in plane_ATOM_n2ofilt['campaign_id'].unique():\n",
    "    for j in plane_ATOM_n2ofilt.loc[plane_ATOM_n2ofilt['campaign_id'] == i].flight_lat_gradient.unique():\n",
    "        with bebi103.stan.disable_logging():\n",
    "        \n",
    "            # the following is copied from what you need for SBC, \n",
    "            # so passing gc_sim data (but doesn't actually need it)\n",
    "\n",
    "            gc_atom_synop_itp_df = gc_atom_synop_n2ofilt_theta_itp_100tropres.loc[(gc_atom_synop_n2ofilt_theta_itp_100tropres['campaign_id'] == i) & \n",
    "                                  (gc_atom_synop_n2ofilt_theta_itp_100tropres['flight_lat_gradient'] == j) & \n",
    "                                  (gc_atom_synop_n2ofilt_theta_itp_100tropres['synoptic_day_number'] <= 7) & \n",
    "                                    (gc_atom_synop_n2ofilt_theta_itp_100tropres['synoptic_day_number'] >= 3)].copy()\n",
    "\n",
    "            plane_df = plane_ATOM_filt_hcnfilt.loc[(plane_ATOM_filt_hcnfilt['campaign_id'] == i) & \n",
    "                               (plane_ATOM_filt_hcnfilt['flight_lat_gradient'] == j)].copy()\n",
    "\n",
    "\n",
    "            # get sigma for likelihood using tropht\n",
    "            max_tropht = np.max(plane_df.tropht.values)\n",
    "            min_tropht = np.min(plane_df.tropht.values)\n",
    "            plane_df['sigma_likelihood'] = np.array([sigma_generator_function2(x,max_tropht,min_tropht,.8,.14) for x in plane_df.tropht.values])\n",
    "\n",
    "            tau_means = tao_mean(.05,.05,get_delta_t(gc_atom_synop_itp_df['synoptic_day_number'].values, 5))\n",
    "\n",
    "            # get replicates of plane gas measurements for input to hierarchical model\n",
    "            # and add it to the gc df\n",
    "            plane_df_sorted = plane_df.sort_values(by=['theta'])\n",
    "            plane_c3h8 = [[x for x in plane_df_sorted.c3h8.values] for i in gc_atom_synop_itp_df['synoptic_day_number'].unique()]\n",
    "            plane_c3h8_flat = [item for sublist in plane_c3h8 for item in sublist]        \n",
    "            gc_atom_synop_itp_df2 = gc_atom_synop_itp_df.copy()\n",
    "            gc_atom_synop_itp_df2['plane_c3h8'] = plane_c3h8_flat\n",
    "            gc_atom_synop_itp_df2['tau_means'] = tau_means\n",
    "\n",
    "            # get replicates of plane sigma for likelihood estimates for input to \n",
    "            # the hierarchical model and add it to the gc df\n",
    "            plane_df_sorted = plane_df.sort_values(by=['theta'])\n",
    "            plane_sigma = [[x for x in plane_df_sorted.sigma_likelihood.values] for i in gc_atom_synop_itp_df['synoptic_day_number'].unique()]\n",
    "            plane_sigma_flat = [item for sublist in plane_sigma for item in sublist]        \n",
    "            gc_atom_synop_itp_df2['plane_sigma'] = plane_sigma_flat\n",
    "\n",
    "\n",
    "            # get data onto same dataframe for input to model\n",
    "            df = gc_atom_synop_itp_df2.copy().reset_index()\n",
    "            tmp_dict, df = bebi103.stan.df_to_datadict_hier(\n",
    "            df, level_cols=[\"synoptic_day_number\"], data_cols=[\"theta\",\"c3h8\",\"plane_c3h8\",\"plane_sigma\",\"tau_means\"]\n",
    "\n",
    "            )\n",
    "            # fix the indexing output\n",
    "            data = {'N':tmp_dict['N'], 'J_1':tmp_dict['J_1'], 'index_1':tmp_dict['index_1'], \n",
    "                        'theta':df.sort_index().theta.values, 'gc_sim':df.sort_index().c3h8.values, \n",
    "                        'plane':df.sort_index().plane_c3h8.values, 'sigma':df.sort_index().plane_sigma.values,\n",
    "                        'tau_':df.sort_index().tau_means.values}\n",
    "            # fix naming \n",
    "            # data['gc_sim'] = data.pop('c2h6')\n",
    "            # data['plane'] = data.pop('plane_c2h6')\n",
    "            data_prior_pred = data.copy()\n",
    "            data_prior_pred.update({   \n",
    "              \"mu_beta\":.7,\n",
    "              \"sigma_beta\":.2\n",
    "            })\n",
    "\n",
    "            # run model \n",
    "            samples = sm.sample(data=data_prior_pred, iter_warmup = 2000, iter_sampling=1000, adapt_delta = 0.99, chains=4)\n",
    "            samples = az.from_cmdstanpy(posterior=samples, posterior_predictive=[\"gc_sim_ppc\"])  \n",
    "\n",
    "            # ppc\n",
    "            gc_sim_ppc = samples.posterior_predictive[\"gc_sim_ppc\"].stack({\"sample\": (\"chain\", \"draw\")})\n",
    "            gc_sim_ppc = gc_sim_ppc.transpose('sample', 'gc_sim_ppc_dim_0')\n",
    "\n",
    "            alpha_samples.append(samples.posterior.alpha.values.flatten())\n",
    "            CI_alpha = np.percentile(samples.posterior.alpha.values.flatten(), [2.5, 50, 97.5])\n",
    "\n",
    "            # save out data \n",
    "            if j == -1:\n",
    "                curtain_name = 'pacific'\n",
    "            elif j == 1:\n",
    "                curtain_name = 'atlantic'\n",
    "            c_name.append('ATom')\n",
    "            c_id.append(i)\n",
    "            c_flg.append(curtain_name)\n",
    "            species.append('c3h8')\n",
    "            alpha_lower.append(CI_alpha[0])\n",
    "            alpha_median.append(CI_alpha[1])\n",
    "            alpha_upper.append(CI_alpha[2])\n",
    "            \n",
    "\n",
    "            # alpha_1 pertaining to the actual plane flight \n",
    "    #         alpha_path.append(np.percentile(samples.posterior.alpha_1[:,:,2].values.flatten(), [10, 50, 90])[1])\n",
    "            alpha_path.append(np.mean(samples.posterior.alpha_1[:,:,2].values.flatten()))\n",
    "            ppc_l.append(gc_sim_ppc)\n",
    "\n",
    "            # print model diagnostics for each run\n",
    "            print(str('ATom' + ' ' + str(i) + ' ' + curtain_name))\n",
    "            bebi103.stan.check_all_diagnostics(samples)\n",
    "\n",
    "atom_hierarch_scaling = pd.DataFrame()\n",
    "atom_hierarch_scaling['campaign_name'] = c_name\n",
    "atom_hierarch_scaling['campaign_id'] = c_id\n",
    "atom_hierarch_scaling['curtain'] = c_flg\n",
    "atom_hierarch_scaling['species'] = species\n",
    "atom_hierarch_scaling['alpha_lower'] = alpha_lower\n",
    "atom_hierarch_scaling['alpha_median'] = alpha_median\n",
    "atom_hierarch_scaling['alpha_upper'] = alpha_upper\n",
    "atom_hierarch_scaling['alpha_path'] = alpha_path\n",
    "atom_ppc = ppc_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff199b3b-b083-4650-8127-117f3953d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = [0,1,2,3,4,5,6,7]\n",
    "df_alpha_only = pd.DataFrame()\n",
    "df_alpha_only['campaign_name'] = ['atom' for curtain in alpha_samples for val in curtain]\n",
    "df_alpha_only['campaign_id'] = [atom_hierarch_scaling['campaign_id'].values[c] for c in counter for campaign in alpha_samples[c]]\n",
    "df_alpha_only['curtain'] = [atom_hierarch_scaling['curtain'].values[c] for c in counter for campaign in alpha_samples[c]]\n",
    "df_alpha_only['species'] = ['C3H8' for curtain in alpha_samples for val in curtain] \n",
    "df_alpha_only['posterior'] = [val for curtain in alpha_samples for val in curtain]\n",
    "df_alpha_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc2fd0-b9bd-4d53-b5c9-76cdfb562d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_only.to_csv('./' + 'c3h8_alpha_hyperparam' + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8eed7-373a-42b3-bcff-d79204f14882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcmc = bebi103.stan.arviz_to_dataframe(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed644e2-10ec-429d-821b-e0f8590b280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcmc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b58dea-9ec5-4e73-b4fd-3c55c0b8bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 400\n",
    "fw = 500\n",
    "title_name = 'Posterior'\n",
    "color = [\"#e78ac3\",\"#8da0cb\",\"black\",\"#fc8d62\",\"#66c2a5\"]\n",
    "p = bokeh.plotting.figure(frame_height=fh, frame_width=fw, title=title_name)\n",
    "legend_it=[]\n",
    "for i in range(0,5):\n",
    "    if i == 0:\n",
    "        synlabel = \"plane path - 2 days\"\n",
    "    elif i == 1:\n",
    "        synlabel = \"plane path - 1 days\"\n",
    "    elif i == 2:\n",
    "        synlabel = \"plane path\"\n",
    "    elif i == 3:\n",
    "        synlabel = \"plane path + 1 days\"\n",
    "    elif i == 4:\n",
    "        synlabel = \"plane path + 2 days\"\n",
    "        \n",
    "    param_name = 'alpha_1[' + str(i) + ']'\n",
    "    c=p.circle(x=df_mcmc['alpha'].values, y=df_mcmc[param_name].values, size=2, fill_alpha=.3, \n",
    "             color=color[i])\n",
    "#     p.circle(x=df_mcmc['alpha'].values, y=df_mcmc[param_name].values, size=2, fill_alpha=.3, \n",
    "#          color=color[i],legend_label=synlabel)\n",
    "    legend_it.append((synlabel, [c]))\n",
    "\n",
    "\n",
    "legend = Legend(items=legend_it)\n",
    "p.add_layout(legend, 'right')\n",
    "p.xaxis.axis_label = \"ùõº\"\n",
    "p.yaxis.axis_label = \"ùõº‚ÇÅ\"\n",
    "p.legend.label_text_font_size = '14pt'\n",
    "# p.legend.location = \"bottom_right\"\n",
    "p.xaxis.axis_label_text_font_size = \"20pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"20pt\"\n",
    "p.xaxis.major_label_text_font_size = \"15pt\"\n",
    "p.yaxis.major_label_text_font_size = \"15pt\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.yaxis.major_tick_line_width = 3\n",
    "p.axis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed75a8-b148-4181-b864-ec36957b86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "confs = []\n",
    "for i in range(len(atom_hierarch_scaling.campaign_name)):\n",
    "#     names.append(str(atom_hierarch_scaling.campaign_name.values[i]) + ' ' +\n",
    "#         str(atom_hierarch_scaling.campaign_id.values[i]) + ' ' +\n",
    "#         str(atom_hierarch_scaling.curtain.values[i]))\n",
    "    confs.append([atom_hierarch_scaling.alpha_lower.values[i], atom_hierarch_scaling.alpha_upper.values[i]])\n",
    "\n",
    "# actually, instead of the actual names, use the season for easier comparison\n",
    "names = ['Summer Pacific 2016','Summer Atlantic 2016','Winter Pacific 2017','Winter Atlantic 2017',\n",
    "         'Fall Pacific 2017','Fall Atlantic 2017','Spring Pacific 2018', 'Spring Atlantic 2018']\n",
    "means = atom_hierarch_scaling.alpha_path.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6da119-aea8-4a3c-b073-ddfa1806f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bokeh.io.show(plot_with_error_bars(means, confs, names, x_axis_label='alpha path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4622ab-c3db-4f18-8b01-8d971adfb111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you used the mean for alpha_path above, \n",
    "# and here you are reporting the median\n",
    "\n",
    "medians = atom_hierarch_scaling.alpha_median.values\n",
    "p = plot_with_error_bars(medians, confs, names, x_axis_label='alpha hyperparam')\n",
    "p.xaxis.axis_label = \"ùõº\"\n",
    "p.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.xaxis.major_label_text_font_size = \"15pt\"\n",
    "p.yaxis.major_label_text_font_size = \"12pt\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.yaxis.major_tick_line_width = 3\n",
    "p.axis.axis_label_text_font_style = 'bold'\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c74b2-6877-4bbc-a009-f16cdaff684f",
   "metadata": {},
   "source": [
    "alpha path is the alpha estimated during the actual plane path time (synoptic replicate #2). alpha hyperparam is the alpha estimated as a result of all the hierarchy synoptic replicates within each campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825b0cd-8011-4330-900d-d83f35596c64",
   "metadata": {},
   "source": [
    "## Simulation - based calibration\n",
    "- Can our model and sampling procedure capture the real parameter values of the generative process?\n",
    "- Can measured data inform the parameters?\n",
    "- Can our sampling technique handle the model and any conceivable data set we can throw at it?\n",
    "\n",
    "1. Draw a parameter set ùúÉÃÉ  out of the prior.\n",
    "2. Use ùúÉÃÉ  to draw a data set ùë¶ÃÉ  out of the likelihood.\n",
    "3. Perform MCMC sampling of the posterior using ùë¶ÃÉ  as if it were the actual measured data set. Draw ùêø MCMC samples of the parameters.\n",
    "4. Do steps 1-3 ùëÅ times (hundreds to a thousand is usually a good number).\n",
    "\n",
    "In step 3, we are using a data set for which we know the underlying parameters that generated it. Because the data were generated using ùúÉÃÉ  as the parameter set, ùúÉÃÉ  is now the ground truth parameter set. So, we can check to see if we uncover the ground truth in the posterior sampling. We can also check diagnostics for each of the trials to make sure the sampler works properly. Furthermore, we can see if the posterior is narrower than the prior, meaning that the data are informing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed6ce8-532a-4b6d-a116-9f1a9ce182f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_sbc_csv = 'df_sbc_hier_atom2_pacific_032121'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ff823-4f96-47a4-a413-0d4f06927c2e",
   "metadata": {},
   "source": [
    "##### I just ran the following below, which means that I used Atom 4 atlantic for this SBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26289b-f7d2-40fd-8ae8-cf63af8c913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores=2\n",
    "df_sbc = bebi103.stan.sbc(\n",
    "    prior_predictive_model=sm_prior_pred,\n",
    "    posterior_model=sm,\n",
    "    prior_predictive_model_data=data_prior_pred,\n",
    "    posterior_model_data=data_prior_pred,\n",
    "    measured_data=[\"gc_sim\"],\n",
    "    var_names=[\"beta_\", \"beta_1\"],\n",
    "    measured_data_dtypes=dict(gc_sim=float),\n",
    "    posterior_predictive_var_names=[\"gc_sim_ppc\"],\n",
    "    sampling_kwargs=dict(iter_warmup = 2000, iter_sampling=1000, adapt_delta = 0.99), \n",
    "    cores=cores,\n",
    "    N=400,\n",
    "    progress_bar=True,\n",
    ")\n",
    "#sampling_kwargs=dict(warmup_iters=2000, sampling_iters=2000),\n",
    "#output_dir='./'\n",
    "df_sbc.to_csv('./' + name_of_sbc_csv + '.csv',  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd98aa-f533-497a-b030-73fe8b796ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sbcr = pd.read_csv('df_sbc_hier_atom2_pacific_032121.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fb80d-d040-4994-82ff-62815ed2d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 400\n",
    "fw = 500\n",
    "\n",
    "color = [\"black\",\"#e78ac3\",\"#8da0cb\",\"#fc8d62\",\"#66c2a5\",'#a55194']\n",
    "p = bokeh.plotting.figure(frame_height=fh, frame_width=fw, title='')\n",
    "legend_it=[]\n",
    "for i in range(0,6):\n",
    "    if i == 0:\n",
    "        data = df_sbcr.loc[df_sbcr['parameter'] == 'beta_']\n",
    "        synlabel = \"Œ≤\"\n",
    "        \n",
    "    else:\n",
    "        data = df_sbcr.loc[df_sbcr['parameter'] == 'beta_1['+str(i-1)+']']\n",
    "        if i == 1:\n",
    "            synlabel = \"Œ≤‚ÇÅ, plane path - 2 days\"\n",
    "        elif i == 2:\n",
    "            synlabel = \"Œ≤‚ÇÅ, plane path - 1 days\"\n",
    "        elif i == 3:\n",
    "            synlabel = \"Œ≤‚ÇÅ, plane path\"\n",
    "        elif i == 4:\n",
    "            synlabel = \"Œ≤‚ÇÅ, plane path + 1 days\"\n",
    "        elif i == 5:\n",
    "            synlabel = \"Œ≤‚ÇÅ, plane path + 2 days\"\n",
    "        \n",
    "   \n",
    "    c=p.circle(x=data['shrinkage'].values, y=data['z_score'].values, size=5, fill_alpha=.3, \n",
    "             color=color[i])\n",
    "    legend_it.append((synlabel, [c]))\n",
    "\n",
    "\n",
    "legend = Legend(items=legend_it)\n",
    "p.add_layout(legend, 'right')\n",
    "p.xaxis.axis_label = \"shrinkage\"\n",
    "p.yaxis.axis_label = \"z-score\"\n",
    "p.legend.label_text_font_size = '14pt'\n",
    "p.xaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"16pt\"\n",
    "p.xaxis.major_label_text_font_size = \"15pt\"\n",
    "p.yaxis.major_label_text_font_size = \"15pt\"\n",
    "p.xaxis.major_tick_line_width = 3\n",
    "p.yaxis.major_tick_line_width = 3\n",
    "p.axis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
